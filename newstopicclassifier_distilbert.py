# -*- coding: utf-8 -*-
"""NewsTopicClassifier_DistilBERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jJ_u9Iv9IHyh6cg167lSuYgfI7d7NpXx

# News Topic Classifier Using BERT
## Objective:
Fine-tune a transformer model (e.g., BERT) to classify news headlines into topic categories.
## Dataset:
AG News Dataset (Available on Hugging Face Datasets)
"""

!pip install transformers datasets torch scikit-learn pyngrok

!pip install streamlit

from datasets import load_dataset
#from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
import torch
from sklearn.metrics import accuracy_score, f1_score
from pyngrok import ngrok
import os
import streamlit as st

from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments

# Load the AG News dataset
dataset = load_dataset("ag_news")

# Check the first few examples
print(dataset['train'][0])

# Load DistilBERT tokenizer and model (smaller than BERT, so faster)
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4)

# Tokenization function
def tokenize_function(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

# Tokenize train and test datasets
train_dataset = dataset['train'].map(tokenize_function, batched=True)
test_dataset = dataset['test'].map(tokenize_function, batched=True)

# Set training arguments for faster training
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=1,              # Reduce to 1 epoch
    per_device_train_batch_size=4,   # Reduce batch size
    per_device_eval_batch_size=8,    # Reduce batch size
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    # evaluation_strategy="steps",     # Evaluate after a fixed number of steps
    # eval_steps=500                   # Evaluate after 500 steps
)

# Compute metrics
def compute_metrics(p):
    preds = p.predictions.argmax(axis=-1)
    labels = p.label_ids
    acc = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds, average='weighted')
    return {'accuracy': acc, 'f1': f1}

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Evaluate the model
results = trainer.evaluate()

# Display the evaluation metrics
print(f"Accuracy: {results['eval_accuracy']}")
print(f"F1-Score: {results['eval_f1']}")

import numpy as np
from sklearn.metrics import confusion_matrix

predictions = trainer.predict(test_dataset)
y_true = predictions.label_ids
y_pred = np.argmax(predictions.predictions, axis=1)

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["World","Sports","Business","Sci/Tech"],
            yticklabels=["World","Sports","Business","Sci/Tech"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

model.save_pretrained("news_bert_model")
tokenizer.save_pretrained("news_bert_model")

from transformers import pipeline

classifier = pipeline(
    "text-classification",
    model="news_bert_model",
    tokenizer="news_bert_model"
)

test_text = "Apple releases a new AI-powered iPhone"
classifier(test_text)

def predict_topic(text):
    result = classifier(text)[0]
    return result["label"], round(result["score"], 3)

predict_topic("NASA launches new satellite into orbit")

label_map = {
    0: "World",
    1: "Sports",
    2: "Business",
    3: "Sci/Tech"
}

def predict_topic(text):
    result = classifier(text)[0]
    label_id = int(result["label"].split("_")[-1])
    return label_map[label_id], round(result["score"], 3)

predict_topic("Microsoft launches new AI product")

import matplotlib.pyplot as plt
import seaborn as sns

labels = dataset["train"]["label"]

plt.figure(figsize=(6,4))
sns.countplot(x=labels, color="purple")
plt.title("Class Distribution in AG News Dataset")
plt.xlabel("Class Label")
plt.ylabel("Count")
plt.show()

sample = "Google announces new AI model for healthcare"
output = classifier(sample)[0]

plt.bar(["Confidence"], [output["score"]])
plt.ylim(0,1)
plt.title(f"Prediction: {output['label']}")
plt.show()

logs = trainer.state.log_history

losses = [log["loss"] for log in logs if "loss" in log]
steps = range(len(losses))

plt.figure(figsize=(6,4))
plt.plot(steps, losses)
plt.xlabel("Training Steps")
plt.ylabel("Loss")
plt.title("Training Loss Curve")
plt.show()